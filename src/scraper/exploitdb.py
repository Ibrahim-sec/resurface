#!/usr/bin/env python3
"""
Exploit-DB Scraper
Scrapes exploits from exploit-db.com for training data.
"""
import json
import time
import re
import requests
from pathlib import Path
from loguru import logger
from typing import Optional

class ExploitDBScraper:
    """Scrapes exploits from Exploit-DB."""
    
    BASE_URL = "https://www.exploit-db.com"
    SEARCH_URL = "https://www.exploit-db.com/search"
    
    # Exploit types we care about (web-focused)
    EXPLOIT_TYPES = {
        "webapps": "Web Application Exploits",
        "sql": "SQL Injection",
        "xss": "Cross-Site Scripting",
    }
    
    def __init__(self, cache_dir: str = "data/exploitdb", rate_limit: float = 1.0):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.rate_limit = rate_limit
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "X-Requested-With": "XMLHttpRequest",
        })
    
    def search_exploits(self, exploit_type: str = "webapps", max_results: int = 1000) -> list[dict]:
        """Search for exploits by type."""
        exploits = []
        page = 0
        per_page = 100
        
        while len(exploits) < max_results:
            params = {
                "type": exploit_type,
                "draw": page + 1,
                "start": page * per_page,
                "length": per_page,
            }
            
            try:
                response = self.session.get(self.SEARCH_URL, params=params)
                if response.status_code != 200:
                    logger.warning(f"Search failed: {response.status_code}")
                    break
                
                data = response.json()
                records = data.get("data", [])
                
                if not records:
                    break
                
                for record in records:
                    exploit_id = record.get("id")
                    if exploit_id:
                        # description is a list: [id, title]
                        desc = record.get("description", [])
                        title = desc[1] if isinstance(desc, list) and len(desc) > 1 else str(desc)
                        platform = record.get("platform", {})
                        platform_name = platform.get("platform", "") if isinstance(platform, dict) else str(platform)
                        
                        exploits.append({
                            "id": exploit_id,
                            "title": self._clean_html(title),
                            "date": record.get("date_published", ""),
                            "platform": platform_name,
                            "type": exploit_type,
                        })
                
                logger.info(f"Page {page + 1}: fetched {len(records)} exploits (total: {len(exploits)})")
                page += 1
                time.sleep(self.rate_limit)
                
            except Exception as e:
                logger.error(f"Search error: {e}")
                break
        
        return exploits[:max_results]
    
    def _clean_html(self, text: str) -> str:
        """Remove HTML tags from text."""
        clean = re.sub(r'<[^>]+>', '', text)
        return clean.strip()
    
    def get_exploit_details(self, exploit_id: str) -> Optional[dict]:
        """Get full exploit details including code."""
        cache_file = self.cache_dir / f"{exploit_id}.json"
        
        # Check cache
        if cache_file.exists():
            with open(cache_file) as f:
                return json.load(f)
        
        try:
            # Get exploit page
            url = f"{self.BASE_URL}/exploits/{exploit_id}"
            response = self.session.get(url)
            
            if response.status_code != 200:
                return None
            
            html = response.text
            
            # Extract exploit code (raw)
            raw_url = f"{self.BASE_URL}/raw/{exploit_id}"
            raw_response = self.session.get(raw_url)
            exploit_code = raw_response.text if raw_response.status_code == 200 else ""
            
            # Parse basic info from page
            title_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html)
            title = title_match.group(1).strip() if title_match else ""
            
            # Extract CVE if present
            cve_match = re.search(r'CVE-\d{4}-\d+', html)
            cve = cve_match.group(0) if cve_match else None
            
            # Extract vulnerability type from code comments
            vuln_type = self._detect_vuln_type(exploit_code, title)
            
            exploit = {
                "id": exploit_id,
                "title": title,
                "cve": cve,
                "vuln_type": vuln_type,
                "code": exploit_code,
                "url": url,
            }
            
            # Cache it
            with open(cache_file, 'w') as f:
                json.dump(exploit, f, indent=2)
            
            return exploit
            
        except Exception as e:
            logger.error(f"Error fetching exploit {exploit_id}: {e}")
            return None
    
    def _detect_vuln_type(self, code: str, title: str) -> str:
        """Detect vulnerability type from code and title."""
        text = (code + " " + title).lower()
        
        if "sql injection" in text or "sqli" in text or "union select" in text:
            return "sqli"
        elif "xss" in text or "cross-site scripting" in text or "<script>" in text:
            return "xss_reflected"
        elif "rce" in text or "remote code execution" in text or "command injection" in text:
            return "rce"
        elif "lfi" in text or "local file inclusion" in text or "path traversal" in text:
            return "path_traversal"
        elif "ssrf" in text or "server-side request forgery" in text:
            return "ssrf"
        elif "xxe" in text or "xml external entity" in text:
            return "xxe"
        elif "csrf" in text or "cross-site request forgery" in text:
            return "csrf"
        elif "auth" in text and ("bypass" in text or "broken" in text):
            return "auth_bypass"
        elif "upload" in text and ("file" in text or "shell" in text):
            return "file_upload"
        elif "deserialization" in text or "unserialize" in text:
            return "deserialization"
        else:
            return "unknown"
    
    def scrape(self, max_exploits: int = 1000, exploit_type: str = "webapps") -> list[dict]:
        """Main scrape function."""
        logger.info(f"Starting Exploit-DB scrape (target: {max_exploits} {exploit_type} exploits)")
        
        # Search for exploits
        exploits = self.search_exploits(exploit_type, max_exploits)
        logger.info(f"Found {len(exploits)} exploit IDs")
        
        full_exploits = []
        for i, exp in enumerate(exploits):
            exploit_id = exp["id"]
            
            details = self.get_exploit_details(exploit_id)
            if details and details.get("code") and len(details["code"]) > 100:
                full_exploits.append(details)
                logger.info(f"âœ… [{len(full_exploits)}] {exploit_id} | {details.get('vuln_type', 'unknown')} | {details.get('title', '')[:50]}")
            
            if (i + 1) % 50 == 0:
                logger.info(f"Progress: {i + 1}/{len(exploits)}, found {len(full_exploits)} full exploits")
            
            time.sleep(self.rate_limit)
            
            if len(full_exploits) >= max_exploits:
                break
        
        logger.info(f"Scrape complete: {len(full_exploits)} exploits with code")
        return full_exploits


def main():
    scraper = ExploitDBScraper()
    exploits = scraper.scrape(max_exploits=100)
    print(f"Scraped {len(exploits)} exploits")


if __name__ == "__main__":
    main()
