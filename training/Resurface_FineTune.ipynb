{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî™ Resurface - Vulnerability Hunter Fine-Tuning\n",
        "\n",
        "**Fully automated** - just click Runtime ‚Üí Run All\n",
        "\n",
        "Fine-tunes Llama 3.1 8B on 236 PortSwigger playbooks.\n",
        "\n",
        "**Baseline:** Llama 3.3 70B without training = 25.9%\n",
        "\n",
        "**Target:** 50%+ after fine-tuning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1Ô∏è‚É£ Setup Environment (2-3 min)\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl huggingface_hub\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2Ô∏è‚É£ Check GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    raise SystemExit(\"GPU required\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3Ô∏è‚É£ Download Training Data from GitHub\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Download training data directly from GitHub\n",
        "TRAIN_URL = \"https://raw.githubusercontent.com/Ibrahim-sec/resurface/master/training/data/train.jsonl\"\n",
        "\n",
        "print(\"Downloading training data...\")\n",
        "urllib.request.urlretrieve(TRAIN_URL, \"train.jsonl\")\n",
        "\n",
        "# Count examples\n",
        "with open(\"train.jsonl\") as f:\n",
        "    num_examples = sum(1 for _ in f)\n",
        "print(f\"‚úÖ Downloaded {num_examples} training examples\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4Ô∏è‚É£ Load Training Data\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_training_data(filepath):\n",
        "    examples = []\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            # Convert to Llama chat format\n",
        "            text = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{data['messages'][0]['content']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{data['messages'][1]['content']}<|eot_id|>\"\"\"\n",
        "            examples.append({\"text\": text})\n",
        "    return Dataset.from_list(examples)\n",
        "\n",
        "dataset = load_training_data(\"train.jsonl\")\n",
        "print(f\"‚úÖ Loaded {len(dataset)} examples\")\n",
        "print(f\"\\nüìù Sample (first 300 chars):\\n{dataset[0]['text'][:300]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5Ô∏è‚É£ Login to Hugging Face (for model access)\n",
        "from huggingface_hub import login, HfFolder\n",
        "\n",
        "# Check if already logged in\n",
        "token = HfFolder.get_token()\n",
        "if token:\n",
        "    print(\"‚úÖ Already logged in to Hugging Face\")\n",
        "else:\n",
        "    print(\"üîë Need Hugging Face token for Llama access\")\n",
        "    print(\"Get yours at: https://huggingface.co/settings/tokens\")\n",
        "    from getpass import getpass\n",
        "    token = getpass(\"Enter HF token: \")\n",
        "    login(token=token)\n",
        "    print(\"‚úÖ Logged in!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6Ô∏è‚É£ Load Model (3-5 min)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"üì• Loading {MODEL_ID}...\")\n",
        "print(\"(This takes 3-5 minutes, be patient)\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7Ô∏è‚É£ Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "print(\"‚úÖ LoRA configured!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8Ô∏è‚É£ Setup Trainer\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./resurface-vuln-hunter\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=\"none\",  # Disable wandb\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=2048,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer ready!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9Ô∏è‚É£ üöÄ TRAIN! (30-60 min)\n",
        "print(\"=\"*50)\n",
        "print(\"üöÄ STARTING FINE-TUNING\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚è±Ô∏è  This takes 30-60 minutes on T4\")\n",
        "print(\"üìä Watch the loss decrease below\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîü Save to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/resurface-vuln-hunter-lora\"\n",
        "trainer.model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(f\"‚úÖ Model saved to Google Drive: {SAVE_PATH}\")\n",
        "print(\"\\nüì• You can download it from Drive and use in Resurface!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1Ô∏è‚É£1Ô∏è‚É£ Test the Model!\n",
        "def generate_exploit(vuln_type, description):\n",
        "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "You are a security researcher testing a web application for vulnerabilities.\n",
        "\n",
        "**Vulnerability Type:** {vuln_type}\n",
        "**Scenario:** {description}\n",
        "\n",
        "Generate a step-by-step exploitation methodology. Include:\n",
        "1. Detailed exploitation steps (numbered)\n",
        "2. Key payloads to use\n",
        "3. Indicators of success<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=800, temperature=0.3, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test on SQL injection\n",
        "print(\"üß™ Testing on SQL Injection scenario...\\n\")\n",
        "result = generate_exploit(\n",
        "    \"sqli\",\n",
        "    \"The application has a product search feature that queries a database. The search term is reflected in the URL.\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Done!\n",
        "\n",
        "## Results\n",
        "- Model saved to: `/content/drive/MyDrive/resurface-vuln-hunter-lora`\n",
        "- Download from Google Drive\n",
        "\n",
        "## Use in Resurface\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, \"path/to/resurface-vuln-hunter-lora\")\n",
        "```\n",
        "\n",
        "## Compare to Baseline\n",
        "- **Before (no training):** 25.9%\n",
        "- **After (fine-tuned):** Run eval to find out! üéØ"
      ],
      "metadata": {}
    }
  ]
}
