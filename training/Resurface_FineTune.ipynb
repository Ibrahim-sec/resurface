{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”ª Resurface Fine-Tuning (Fixed Version)\n",
        "\n",
        "Fine-tunes TinyLlama 1.1B on vulnerability playbooks.\n",
        "\n",
        "**Baseline:** 25.9% (Llama 3.3 70B without training)\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
        "2. Runtime â†’ Run all\n",
        "3. Wait ~20-30 minutes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install dependencies (pinned for Colab Feb 2026)\n",
        "!pip uninstall -y -q transformers\n",
        "!pip install -q transformers==4.40.0 datasets peft accelerate bitsandbytes==0.43.0 trl==0.9.6 huggingface_hub\n",
        "print(\"âœ… Done! Now RESTART RUNTIME (Runtime â†’ Restart runtime) then run from Cell 2\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Check GPU\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"No GPU! Go to Runtime â†’ Change runtime type â†’ GPU\"\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download training data\n",
        "!wget -q https://raw.githubusercontent.com/Ibrahim-sec/resurface/master/training/data/train.jsonl\n",
        "!wc -l train.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Load and format data\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "examples = []\n",
        "with open(\"train.jsonl\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        # Simple format\n",
        "        text = f\"### Human: {data['messages'][0]['content']}\\n\\n### Assistant: {data['messages'][1]['content']}\"\n",
        "        examples.append({\"text\": text})\n",
        "\n",
        "dataset = Dataset.from_list(examples)\n",
        "print(f\"Loaded {len(dataset)} examples\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Load model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(f\"Loading {MODEL}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Setup LoRA\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Train\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Save to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/resurface-lora\"\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(f\"Saved to {SAVE_PATH}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test it!\n",
        "prompt = \"\"\"### Human: You are a security researcher. Vulnerability Type: sqli. Scenario: A search feature that queries a database directly. Generate exploitation steps.\n",
        "\n",
        "### Assistant:\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
